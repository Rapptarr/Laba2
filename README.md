# Создание генеративной модели текста с использованием рекуррентных нейронных сетей. (Лабораторная №2)

## Цель работы :
Разработка и обучение генеративной модели текста на основе рекуррентных нейронных сетей с использованием LSTM-ячеек. Исследование возможностей модели в генерации креативного и качественного текста, а также оценка её способности запоминать и воспроизводить структуру входных данных. Цель также включает в себя тщательный анализ результатов и оптимизацию модели для достижения лучшей производительности в генерации текста.

## Требования.
- Python 3.x
- Google Colab или Jupyter Notebook.
- Стабильное интернет-подключение для работы модели.

## Как работает программа :
1. **Загрузка данных**:
   * Подключаем все необходимые библиотеки.
   * Загружаем файл "train_text.txt"
   * Обучать нашу модель будем сборником стихотворений Джорджа Байрона на русском языке.
    
2. **Готовим данные для сети**:
   * Превращаем текст в индексы (токенизируем). Т.е. достаем все уникальные символы: буквы, пробелы, знаки препинания в тексте и каждому символу присваиваем число.
   * Это будем называть словарем. А числа — индексами.
   * Делаем прямой и обратный словарь. А потом проходимся по всему тексту и превращаем с помощью составленного нами словаря каждый символ в индекс.

3. **Генерируем батчи из текста**:
   * Генерируем из последовательности наших индексов батчи (сразу несколько строк текста) для обучения сети.
   * Будем генерировать сразу обучающую выборку и таргет для нее.

4. **Пишем функцию, которая генерирует текст**:
   * Сеть предсказывает нам вероятности следующей буквы, и мы с помощью этих вероятностей достаем случайно по одной букве.
   * Параметр ```start_text``` нам нужен, чтобы было что-то, для чего предсказывать следующий символ. У нас этот символ по умолчанию — пробел, и задача сети сначала — предсказать следующий символ после пробела. Потом — следующий после этих 2-х символов. И т.д.
   * Параметр ```temp``` — это уровень «случайности» генерируемого текста. Поставим высокую — вероятность каждой буквы будет почти одинакова и текст превратится в случайный набор символов. Поставим низкую — каждый раз будем предсказывать одно и то же и можем зациклиться на одной фразе.

5. **Создаем класс нашей нейросети**:
   * Превращаем каждый символ на входе сети в вектор. Скармливаем эти векторы нашему LSTM слою.
   * Выходы из LSTM слоя пропускаем через Dropout. Этот слой «мешает» сети учиться, чтобы ей сложнее было выучить весть текст.

6. **Создаем нейросеть и обучаем ее**:
   * Параметры нейросети, которые может понадобиться подкрутить:
   * ```hidden_size``` — влияет на сложность сети. Стоит повышать для текстов большого размера. Если выставить большое значение для текста маленького размера, то сеть просто выучит весь текст и будет генерировать его же.
   * ```n_layers``` — позволяет делать несколько LSTM слоев подряд просто меняя эту цифру.
   * ```embedding_size``` — размер обучаемого эмбеддинга. Желательно выставить в несколько раз меньше числа уникальных символов в тексте или примерно такой же.
   * Дальше — стандартный для PyTorch цикл обучения нейросети: выбираем функцию потерь, оптимизатор и настраиваем расписание, по которому меняем шаг оптимизатора. В нашем случае снижаем шаг в 2 раза, если ошибка (loss) не падает 5 шагов подряд.
   * Грубо говоря, здесь мы много раз подаем в нейросеть разные кусочки текста и учим ее делать все меньше и меньше ошибок, когда она предсказывает следующую букву по предыдущему тексту.
   * В процессе обучения будет получаться что-то вроде:
   * ![xxxx](https://sun9-46.userapi.com/impg/NaGU6nmd2TOIy_DPCn6Y35ma0EVw_ZHTUG_4bw/gtE7acEE8KQ.jpg?size=355x737&quality=96&sign=bfd7e7b1fe13a0bc48512cdd62c2277c&type=album).
   * Чем дольше обучается модель, тем лучше и связаннее стихи. Количество так называемых "эпох" или "уровней" обучения не ограничено. В нашем случае гораздо удобнее остановить работу вручную на моменте, когда ошибка (Loss) перестанет снижаться.
   * Как только вы остановили обучение модели, можно приступить к итоговой генерации. Текст генерируем запуском этой функции в самом низу кода.
   * ![xxx](https://sun9-3.userapi.com/impg/C1p7OJex1ofqOKlJJ8GHR90hyqeSa8tnxfpw-Q/WcQFSLM7Eio.jpg?size=207x220&quality=96&sign=893b30a769b913ec3616a0cea4a21e0d&type=album).



  7. **Приёмы**:
   * Мы имеем дело с LSTM. LSTM (Long Short-Term Memory) - это тип рекуррентной нейронной сети (RNN), который широко используется для обработки последовательных данных, таких как текст. Он разработан для преодоления проблемы затухания градиента, которая часто возникает при обучении традиционных RNN на длинных последовательностях.
   * Основная идея LSTM заключается в том, чтобы создать специальную ячейку памяти, которая может хранить информацию на длительный срок и управлять, когда эта информация должна быть забыта или использована. Это позволяет модели эффективно обучаться на длинных зависимостях в данных.
   * У этой сети есть соответствующая особенность: она работает не независимо для каждого символа, а помнит, что к нему раньше приходило на вход. Притом, помнит не все: ненужное он умеет забывать. Такие слои называют рекуррентными и часто используют при работе с последовательностями.
   * Из внешних библиотек нам понадобятся только numpy и pytorch.

## Colab
| Colab                                                                                                                                                                          | Info               |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------ |
| [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1B_WxIMwUgZAaxCThvxmPtakT84I0Cp6B?usp=sharing) | Laba2 |

## Результат.

1. **Посмотрим на генерацию текста.**
   * Остаётся лишь  выделить успешное обучение генеративной модели текста на основе рекуррентных нейронных сетей. Модель, обученная на сборнике стихов, продемонстрировала способность творческой генерации стихотворных текстов, сохраняя характерные черты литературного стиля. Полученные результаты подчеркивают потенциал использования подобных моделей в творческих и прикладных областях, открывая новые перспективы в автоматической генерации текста и обработке естественного языка.
   *  Пример 1
   ![xxxx](https://sun9-45.userapi.com/impg/w20ixYwEnjTNqJunI-ruBWlVXtTuNph_1HunVA/aJnSZ8vYPfc.jpg?size=396x427&quality=96&sign=1951ab2cf997ed8f3f0affb5137366d4&type=album)

   *  Пример 2
   ![xxxx](https://sun9-42.userapi.com/impg/RXaqCvWNstV-7yn5XuDmTd0MguFU_aDcQUJsLQ/pKDInNTXRoc.jpg?size=369x477&quality=96&sign=1895adc81511a526b6ab9de58fcf2f8e&type=album)
